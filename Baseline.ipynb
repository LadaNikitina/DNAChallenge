{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f21c704-c8f6-4b1b-bedc-4ea7b8133cdc",
   "metadata": {},
   "source": [
    "# DNA Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570fe1b4-3eb0-45d0-a22c-83476df67277",
   "metadata": {},
   "source": [
    "## üìö –¶–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞\n",
    "\n",
    "–í–∞—à–∞ –∑–∞–¥–∞—á–∞ ‚Äî –ø–æ—Å—Ç—Ä–æ–∏—Ç—å **–≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è (—ç–º–±–µ–¥–¥–∏–Ω–≥–∏) –î–ù–ö-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π**, –∏ –∑–∞—Ç–µ–º –æ–±—É—á–∏—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã, —á—Ç–æ–±—ã –¥–æ—Å—Ç–∏—á—å **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ 5 –±–∏–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö** –∏–∑ –±–µ–Ω—á–º–∞—Ä–∫–∞ [Nucleotide Transformer Benchmark](https://huggingface.co/datasets/InstaDeepAI/nucleotide_transformer_downstream_tasks).\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ –î–∞—Ç–∞—Å–µ—Ç—ã –∏ –º–µ—Ç—Ä–∏–∫–∏\n",
    "\n",
    "–í—ã –±—É–¥–µ—Ç–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –ø—è—Ç—å—é –ø–æ–¥–∑–∞–¥–∞—á–∞–º–∏ –∏–∑ –±–µ–Ω—á–º–∞—Ä–∫–∞. –î–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ —É–∫–∞–∑–∞–Ω–∞ **–æ—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞**, –ø–æ –∫–æ—Ç–æ—Ä–æ–π –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏:\n",
    "\n",
    "---\n",
    "\n",
    "## üß¨ –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å–º—ã—Å–ª –∑–∞–¥–∞—á\n",
    "\n",
    "| ‚Ññ  | –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏      | –ú–µ—Ç—Ä–∏–∫–∞ –æ—Ü–µ–Ω–∫–∏ | –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ |\n",
    "|----|----------------------|----------------|-------------------------|\n",
    "| 1  | **promoter_all**     | F1-score       | **–ü—Ä–æ–º–æ—Ç–µ—Ä—ã** ‚Äî —É—á–∞—Å—Ç–∫–∏ –î–ù–ö, —Å –∫–æ—Ç–æ—Ä—ã—Ö –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –≥–µ–Ω–∞. –≠—Ç–∏ —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –≥–µ–Ω–∞ –∏ —Å–ª—É–∂–∞—Ç —è–∫–æ—Ä–µ–º –¥–ª—è –†–ù–ö-–ø–æ–ª–∏–º–µ—Ä–∞–∑—ã. –ó–∞–¥–∞—á–∞ –≤–∞–∂–Ω–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ–≥–æ, –∫–∞–∫–∏–µ —É—á–∞—Å—Ç–∫–∏ –≥–µ–Ω–æ–º–∞ –∞–∫—Ç–∏–≤–Ω–æ —ç–∫—Å–ø—Ä–µ—Å—Å–∏—Ä—É—é—Ç—Å—è. |\n",
    "| 2  | **enhancers**        | MCC            | **–≠–Ω—Ö–∞–Ω—Å–µ—Ä—ã** ‚Äî —É—á–∞—Å—Ç–∫–∏ –î–ù–ö, —É—Å–∏–ª–∏–≤–∞—é—â–∏–µ —ç–∫—Å–ø—Ä–µ—Å—Å–∏—é –≥–µ–Ω–æ–≤. –û–Ω–∏ –º–æ–≥—É—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –¥–∞–ª–µ–∫–æ –æ—Ç —Å–∞–º–æ–≥–æ –≥–µ–Ω–∞ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –ø—Ä–æ–º–æ—Ç–µ—Ä–æ–º —á–µ—Ä–µ–∑ 3D-—Å–≤—ë—Ä—Ç–∫—É —Ö—Ä–æ–º–∞—Ç–∏–Ω–∞. –≠–Ω—Ö–∞–Ω—Å–µ—Ä—ã –∏–≥—Ä–∞—é—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –≤ —Ç–∫–∞–Ω–µ—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–π —ç–∫—Å–ø—Ä–µ—Å—Å–∏–∏. –ó–∞–¥–∞—á–∞ —Å–ª–æ–∂–Ω–∞—è –∏–∑-–∑–∞ –≤–∞—Ä–∏–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏ —ç–Ω—Ö–∞–Ω—Å–µ—Ä–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. |\n",
    "| 3  | **splice_sites_all** | Accuracy       | **Splice sites (—Å–∞–π—Ç—ã —Å–ø–ª–∞–π—Å–∏–Ω–≥–∞)** ‚Äî –≥—Ä–∞–Ω–∏—Ü—ã –º–µ–∂–¥—É —ç–∫–∑–æ–Ω–∞–º–∏ –∏ –∏–Ω—Ç—Ä–æ–Ω–∞–º–∏ –≤ –ø—Ä–µ-–º–†–ù–ö. –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç—Ç–∏—Ö —Å–∞–π—Ç–æ–≤ –≤–∞–∂–Ω–æ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å—Ç—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–π. –û—à–∏–±–∫–∏ –≤ —Å–ø–ª–∞–π—Å–∏–Ω–≥–µ —á–∞—Å—Ç–æ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è–º. –ó–∞–¥–∞—á–∞ ‚Äî –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –≥–¥–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è —Å–ø–ª–∞–π—Å–∏–Ω–≥. |\n",
    "| 4  | **H3**               | MCC            | –ú–µ—Ç–∫–∞ **H3 (–≥–∏—Å—Ç–æ–Ω H3, –Ω–∞–ø—Ä–∏–º–µ—Ä, H3K4me3)** —Å–≤—è–∑–∞–Ω–∞ —Å **–∞–∫—Ç–∏–≤–Ω—ã–º–∏ –ø—Ä–æ–º–æ—Ç–µ—Ä–∞–º–∏** –∏ **—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã–º–∏ —Ä–µ–≥–∏–æ–Ω–∞–º–∏**. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–∞–∫–∏—Ö —ç–ø–∏–≥–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–∫ –ø–æ –î–ù–ö-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ —Ä–µ–≥–∏–æ–Ω—ã –º–æ–≥—É—Ç –±—ã—Ç—å \"–≤–∫–ª—é—á–µ–Ω—ã\" –∏–ª–∏ \"–≤—ã–∫–ª—é—á–µ–Ω—ã\" –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ö—Ä–æ–º–∞—Ç–∏–Ω–∞. |\n",
    "| 5  | **H4**               | MCC            | **H4** ‚Äî –µ—â—ë –æ–¥–∏–Ω –≥–∏—Å—Ç–æ–Ω, —É—á–∞—Å—Ç–≤—É—é—â–∏–π –≤ **—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –Ω—É–∫–ª–µ–æ—Å–æ–º** –∏ —É–ø–∞–∫–æ–≤–∫–µ –î–ù–ö. –ï–≥–æ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–∞–∫–∂–µ –∏–≥—Ä–∞—é—Ç —Ä–æ–ª—å –≤ —Ä–µ–≥—É–ª—è—Ü–∏–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏. –ó–∞–¥–∞—á–∞ ‚Äî –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —É—á–∞—Å—Ç–∫–∏ –î–ù–ö, –∫–æ—Ç–æ—Ä—ã–µ, –≤–µ—Ä–æ—è—Ç–Ω–æ, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç —Å –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º H4, —á—Ç–æ –¥–∞—ë—Ç –∫–ª—é—á –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ö—Ä–æ–º–∞—Ç–∏–Ω–∞. |\n",
    "\n",
    "---\n",
    "\n",
    "## üìà –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "\n",
    "–ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ñ–æ—Ä–º—É–ª—ã –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏:\n",
    "\n",
    "### 1. **F1-score (–≥–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ Precision –∏ Recall)**\n",
    "\n",
    "F1-score –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–∞ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–æ **—Å–∏–ª—å–Ω—ã–º –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤**.\n",
    "\n",
    "$$\n",
    "\\text{F1} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}, \\quad \\text{–≥–¥–µ:}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}, \\quad \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "- **TP** ‚Äî –∏—Å—Ç–∏–Ω–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ (True Positives)\n",
    "- **FP** ‚Äî –ª–æ–∂–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ (False Positives)\n",
    "- **FN** ‚Äî –ª–æ–∂–Ω–æ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ (False Negatives)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **MCC (Matthews Correlation Coefficient)**\n",
    "\n",
    "MCC ‚Äî –º–µ—Ç—Ä–∏–∫–∞, –ø–æ–¥—Ö–æ–¥—è—â–∞—è –ø—Ä–∏ —Å–∏–ª—å–Ω–æ–π –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤. –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç `-1` –¥–æ `1`:\n",
    "\n",
    "$$\n",
    "\\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
    "$$\n",
    "\n",
    "- `1` ‚Äî –∏–¥–µ–∞–ª—å–Ω–æ\n",
    "- `0` ‚Äî —Å–ª—É—á–∞–π–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "- `-1` ‚Äî –ø–æ–ª–Ω–æ–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Accuracy (–¥–æ–ª—è –≤–µ—Ä–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤)**\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –∫–æ–≥–¥–∞ –∫–ª–∞—Å—Å—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏\n",
    "\n",
    "### üì¶ –î–∞–Ω–Ω—ã–µ\n",
    "\n",
    "–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∏–∑ HuggingFace:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"InstaDeepAI/nucleotide_transformer_downstream_tasks\", \"promoter_all\")\n",
    "```\n",
    "\n",
    "–ö–∞–∂–¥–∞—è –∑–∞–¥–∞—á–∞ –≤–∫–ª—é—á–∞–µ—Ç –ø–æ–ª—è:\n",
    "- `\"sequence\"` ‚Äî —Å—Ç—Ä–æ–∫–∞ –∏–∑ —Å–∏–º–≤–æ–ª–æ–≤ {A, T, G, C}\n",
    "- `\"label\"` ‚Äî —Ç–∞—Ä–≥–µ—Ç\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ –ë–∞–∑–æ–≤–∞—è —Å—Ö–µ–º–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
    "\n",
    "1. üì• –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ\n",
    "2. üß¨ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –≤–µ–∫—Ç–æ—Ä—ã (—ç–º–±–µ–¥–¥–∏–Ω–≥–∏).\n",
    "3. ü§ñ –û–±—É—á–∏—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä.\n",
    "4. üìä –ü–æ—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫—É –Ω–∞ test.\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ –ó–∞–¥–∞–Ω–∏–µ\n",
    "\n",
    "–ü–æ—Å—Ç—Ä–æ–π—Ç–µ pipeline, –∫–æ—Ç–æ—Ä—ã–π –¥–∞—ë—Ç **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –Ω–∞ –≤—Å–µ—Ö 5 –∑–∞–¥–∞—á–∞—Ö**, –ø—Ä–∏–º–µ–Ω—è—è —Ä–∞–∑–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –î–ù–ö-—Ü–µ–ø–æ—á–µ–∫ –∏ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ò—Ç–æ–≥–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ—à–µ–Ω–∏–π –±—É–¥–µ—Ç –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É –∑–Ω–∞—á–µ–Ω–∏—é –≤—Å–µ—Ö 5 –º–µ—Ç—Ä–∏–∫."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d9b1a51-129a-4ac4-a688-c97369a0dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score, accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15a76ea-b2db-4ffd-a43c-46d397ba94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b77dd8-79f1-49c8-9e25-62203d215da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ Dataset –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ –î–ù–ö\n",
    "class NTDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, k=6):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.k = k\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def _to_kmers(self, seq):\n",
    "        return \" \".join([seq[i:i+self.k] for i in range(len(seq) - self.k + 1)])\n",
    "\n",
    "    # –ï—Å–ª–∏ –Ω—É–∂–Ω—ã k-–º–µ—Ä—ã\n",
    "    # def __getitem__(self, idx):\n",
    "    #     kmers = self._to_kmers(self.sequences[idx])\n",
    "    #     return kmers, self.labels[idx]\n",
    "\n",
    "    # –ï—Å–ª–∏ –Ω—É–∂–Ω—ã –Ω–µ k-–º–µ—Ä—ã, –∞ –∏—Å—Ö–æ–¥–Ω–∞—è —Ü–µ–ø–æ—á–∫–∞\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        return sequence, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8873064d-94f3-4e10-a629-ff7ca901384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nucleotide_transformer(batch_size, valid_split=-1, dataset_name='enhancers', split_state=42):\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∏ —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö\n",
    "    train_dataset = load_dataset(\n",
    "        \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n",
    "        dataset_name,\n",
    "        split=\"train\",\n",
    "        streaming=False,\n",
    "    )\n",
    "\n",
    "    test_dataset = load_dataset(\n",
    "        \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n",
    "        dataset_name,\n",
    "        split=\"test\",\n",
    "        streaming=False,\n",
    "    )\n",
    "\n",
    "    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –º–µ—Ç–∫–∏\n",
    "    train_sequences = train_dataset['sequence']\n",
    "    train_labels = train_dataset['label']\n",
    "\n",
    "    # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä, —Ä–∞–∑–±–∏–≤–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    if valid_split > 0:\n",
    "        train_sequences, validation_sequences, train_labels, validation_labels = train_test_split(\n",
    "            train_sequences, train_labels, test_size=valid_split, random_state=split_state\n",
    "        )\n",
    "\n",
    "    test_sequences = test_dataset['sequence']\n",
    "    test_labels = test_dataset['label']\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç—ã –∫–ª–∞—Å—Å–∞ NTDataset\n",
    "    train_dataset = NTDataset(train_sequences, train_labels)\n",
    "    test_dataset = NTDataset(test_sequences, test_labels)\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º DataLoader'—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ\n",
    "    if valid_split > 0:\n",
    "        validation_dataset = NTDataset(validation_sequences, validation_labels)\n",
    "        valid_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        \n",
    "        print(f\"Train: {len(train_loader.dataset)}, Validation: {len(valid_loader.dataset)}, Test: {len(test_loader.dataset)}\")\n",
    "        return train_loader, valid_loader, test_loader\n",
    "\n",
    "    # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é, –≤—ã–≤–æ–¥–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤\n",
    "    print(f\"Train: {len(train_loader.dataset)}, Test: {len(test_loader.dataset)}\")\n",
    "    return train_loader, None, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d499802e-3c5e-496f-bf02-1c637fd1f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ DNA-BERT-2\n",
    "def get_dnabert2_embeddings(dataloader, tokenizer, model, pooling=\"mean\"):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    model.to(device)\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        sequences, lbls = batch  # —É–∂–µ —á–∏—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏\n",
    "\n",
    "        tokens = tokenizer(\n",
    "            list(sequences),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "            hidden_states = outputs[0]  # shape: [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "        if pooling == \"mean\":\n",
    "            pooled = hidden_states.mean(dim=1)  # [batch_size, hidden_dim]\n",
    "        elif pooling == \"max\":\n",
    "            pooled = hidden_states.max(dim=1)[0]  # [batch_size, hidden_dim]\n",
    "        else:\n",
    "            raise ValueError(\"pooling must be 'mean' or 'max'\")\n",
    "\n",
    "        embeddings.append(pooled.cpu().numpy())\n",
    "        labels.append(lbls.numpy())\n",
    "\n",
    "    return np.vstack(embeddings), np.concatenate(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a048db-400c-49f4-a74d-477554915e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—É—á–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö DNA-BERT\n",
    "def classify_with_dnabert(dataset_name, metric, embedding_fn = get_dnabert2_embeddings):\n",
    "    # –ü–æ—Å–∫–æ–ª—å–∫—É –≤ CatBoost –Ω–µ—Ç Early Stopping –ø–æ –º–µ—Ç—Ä–∏–∫–µ MCC, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ F1\n",
    "    eval_metric = \"F1\" if metric == \"MCC\" else metric\n",
    "        \n",
    "    batch_size = 128 # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –ø–æ–º–µ–Ω—å—à–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "    train_loader, valid_loader, test_loader = load_nucleotide_transformer(batch_size, valid_split=0.1, dataset_name=dataset_name)\n",
    "\n",
    "    # DNA-BERT2: 117M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "    model_name = \"zhihan1996/DNABERT-2-117M\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    config = BertConfig.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name, config=config, trust_remote_code=True)\n",
    "    model.eval()\n",
    "\n",
    "    X_train, y_train = embedding_fn(train_loader, tokenizer, model)\n",
    "    X_valid, y_valid = embedding_fn(valid_loader, tokenizer, model)\n",
    "    X_test, y_test = embedding_fn(test_loader, tokenizer, model)\n",
    "\n",
    "    clf = CatBoostClassifier(\n",
    "        iterations=3_000,\n",
    "        learning_rate=0.02,\n",
    "        depth=4,\n",
    "        task_type=\"GPU\",\n",
    "        eval_metric=eval_metric,\n",
    "        early_stopping_rounds=100,\n",
    "        use_best_model=True,\n",
    "        verbose=50\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train, y_train, eval_set=(X_valid, y_valid) if valid_loader else None)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    if metric == \"F1\":\n",
    "        metric = f1_score(y_test, y_pred, average=\"binary\")\n",
    "        print(f\"F1 Score: {metric:.4f}\")\n",
    "    elif metric == \"Accuracy\":\n",
    "        metric = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {metric:.4f}\")\n",
    "    else:\n",
    "        metric = matthews_corrcoef(y_test, y_pred)\n",
    "        print(f\"MCC: {metric:.4f}\")\n",
    "\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8332527e-3832-4e57-b80a-66303259e6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 47948, Validation: 5328, Test: 5920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ledneva/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [02:02<00:00,  3.05it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:13<00:00,  3.06it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:15<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.8042558\ttest: 0.8047600\tbest: 0.8047600 (0)\ttotal: 57.3ms\tremaining: 2m 51s\n",
      "50:\tlearn: 0.8738135\ttest: 0.8755162\tbest: 0.8755162 (50)\ttotal: 1.59s\tremaining: 1m 32s\n",
      "100:\tlearn: 0.8960472\ttest: 0.8952530\tbest: 0.8954697 (99)\ttotal: 3.09s\tremaining: 1m 28s\n",
      "150:\tlearn: 0.9067555\ttest: 0.9069949\tbest: 0.9069949 (148)\ttotal: 4.59s\tremaining: 1m 26s\n",
      "200:\tlearn: 0.9144889\ttest: 0.9131620\tbest: 0.9139073 (198)\ttotal: 6.08s\tremaining: 1m 24s\n",
      "250:\tlearn: 0.9198816\ttest: 0.9177203\tbest: 0.9181730 (245)\ttotal: 7.58s\tremaining: 1m 23s\n",
      "300:\tlearn: 0.9228594\ttest: 0.9205362\tbest: 0.9205362 (300)\ttotal: 9.1s\tremaining: 1m 21s\n",
      "350:\tlearn: 0.9254229\ttest: 0.9226894\tbest: 0.9236138 (340)\ttotal: 10.6s\tremaining: 1m 20s\n",
      "400:\tlearn: 0.9272378\ttest: 0.9261693\tbest: 0.9265559 (396)\ttotal: 12.1s\tremaining: 1m 18s\n",
      "450:\tlearn: 0.9295516\ttest: 0.9270270\tbest: 0.9272060 (447)\ttotal: 13.7s\tremaining: 1m 17s\n",
      "500:\tlearn: 0.9314233\ttest: 0.9275922\tbest: 0.9285162 (488)\ttotal: 15.2s\tremaining: 1m 15s\n",
      "550:\tlearn: 0.9328252\ttest: 0.9285438\tbest: 0.9287507 (528)\ttotal: 16.8s\tremaining: 1m 14s\n",
      "600:\tlearn: 0.9341417\ttest: 0.9295775\tbest: 0.9295775 (600)\ttotal: 18.3s\tremaining: 1m 12s\n",
      "650:\tlearn: 0.9357198\ttest: 0.9299904\tbest: 0.9303492 (628)\ttotal: 19.8s\tremaining: 1m 11s\n",
      "700:\tlearn: 0.9366051\ttest: 0.9306091\tbest: 0.9308152 (684)\ttotal: 21.3s\tremaining: 1m 10s\n",
      "750:\tlearn: 0.9380095\ttest: 0.9301967\tbest: 0.9308152 (684)\ttotal: 22.9s\tremaining: 1m 8s\n",
      "bestTest = 0.930815186\n",
      "bestIteration = 684\n",
      "Shrink model to first 685 iterations.\n",
      "F1 Score: 0.9249\n",
      "Train: 13471, Validation: 1497, Test: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ledneva/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 106/106 [00:22<00:00,  4.80it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:02<00:00,  4.64it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.7689612\ttest: 0.7807768\tbest: 0.7807768 (0)\ttotal: 27.4ms\tremaining: 1m 22s\n",
      "50:\tlearn: 0.8065608\ttest: 0.8277669\tbest: 0.8283093 (43)\ttotal: 1.29s\tremaining: 1m 14s\n",
      "100:\tlearn: 0.8206790\ttest: 0.8389218\tbest: 0.8409987 (98)\ttotal: 2.56s\tremaining: 1m 13s\n",
      "150:\tlearn: 0.8338983\ttest: 0.8509521\tbest: 0.8517060 (148)\ttotal: 3.83s\tremaining: 1m 12s\n",
      "200:\tlearn: 0.8420742\ttest: 0.8550629\tbest: 0.8558201 (194)\ttotal: 5.1s\tremaining: 1m 11s\n",
      "250:\tlearn: 0.8497822\ttest: 0.8569536\tbest: 0.8584656 (224)\ttotal: 6.38s\tremaining: 1m 9s\n",
      "300:\tlearn: 0.8548637\ttest: 0.8687003\tbest: 0.8687003 (300)\ttotal: 7.65s\tremaining: 1m 8s\n",
      "350:\tlearn: 0.8610270\ttest: 0.8683511\tbest: 0.8692767 (320)\ttotal: 8.9s\tremaining: 1m 7s\n",
      "400:\tlearn: 0.8658573\ttest: 0.8734261\tbest: 0.8740053 (399)\ttotal: 10.2s\tremaining: 1m 5s\n",
      "450:\tlearn: 0.8706405\ttest: 0.8740053\tbest: 0.8759124 (413)\ttotal: 11.4s\tremaining: 1m 4s\n",
      "500:\tlearn: 0.8746386\ttest: 0.8770764\tbest: 0.8785667 (482)\ttotal: 12.7s\tremaining: 1m 3s\n",
      "550:\tlearn: 0.8774844\ttest: 0.8834111\tbest: 0.8834111 (545)\ttotal: 13.9s\tremaining: 1m 1s\n",
      "600:\tlearn: 0.8799881\ttest: 0.8825100\tbest: 0.8847435 (568)\ttotal: 15.2s\tremaining: 1m\n",
      "650:\tlearn: 0.8832517\ttest: 0.8838451\tbest: 0.8851802 (640)\ttotal: 16.4s\tremaining: 59.3s\n",
      "700:\tlearn: 0.8859519\ttest: 0.8832555\tbest: 0.8851802 (640)\ttotal: 17.7s\tremaining: 58.2s\n",
      "bestTest = 0.8851802403\n",
      "bestIteration = 640\n",
      "Shrink model to first 641 iterations.\n",
      "MCC: 0.4453\n",
      "Train: 24300, Validation: 2700, Test: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ledneva/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 190/190 [01:17<00:00,  2.46it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.52it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:09<00:00,  2.53it/s]\n",
      "Warning: less than 75% GPU memory available for training. Free: 10383.125 Total: 16269.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4313169\ttest: 0.4229630\tbest: 0.4229630 (0)\ttotal: 24ms\tremaining: 1m 12s\n",
      "50:\tlearn: 0.4623045\ttest: 0.4574074\tbest: 0.4611111 (16)\ttotal: 360ms\tremaining: 20.8s\n",
      "100:\tlearn: 0.4711111\ttest: 0.4666667\tbest: 0.4700000 (64)\ttotal: 716ms\tremaining: 20.5s\n",
      "150:\tlearn: 0.4827984\ttest: 0.4685185\tbest: 0.4707407 (130)\ttotal: 1.08s\tremaining: 20.3s\n",
      "200:\tlearn: 0.4917695\ttest: 0.4677778\tbest: 0.4714815 (173)\ttotal: 1.46s\tremaining: 20.3s\n",
      "250:\tlearn: 0.5012346\ttest: 0.4625926\tbest: 0.4714815 (173)\ttotal: 1.83s\tremaining: 20s\n",
      "bestTest = 0.4714814815\n",
      "bestIteration = 173\n",
      "Shrink model to first 174 iterations.\n",
      "Accuracy: 0.4607\n",
      "Train: 12121, Validation: 1347, Test: 1497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ledneva/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:48<00:00,  1.97it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:05<00:00,  1.98it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:05<00:00,  2.02it/s]\n",
      "Warning: less than 75% GPU memory available for training. Free: 10381.125 Total: 16269.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.7980851\ttest: 0.8034682\tbest: 0.8034682 (0)\ttotal: 23.7ms\tremaining: 1m 11s\n",
      "50:\tlearn: 0.8184751\ttest: 0.8292683\tbest: 0.8292683 (48)\ttotal: 1.15s\tremaining: 1m 6s\n",
      "100:\tlearn: 0.8259879\ttest: 0.8359088\tbest: 0.8376194 (97)\ttotal: 2.25s\tremaining: 1m 4s\n",
      "150:\tlearn: 0.8338464\ttest: 0.8416422\tbest: 0.8424908 (134)\ttotal: 3.36s\tremaining: 1m 3s\n",
      "200:\tlearn: 0.8394514\ttest: 0.8421829\tbest: 0.8434974 (168)\ttotal: 4.47s\tremaining: 1m 2s\n",
      "250:\tlearn: 0.8435688\ttest: 0.8428044\tbest: 0.8445099 (232)\ttotal: 5.59s\tremaining: 1m 1s\n",
      "300:\tlearn: 0.8472421\ttest: 0.8491124\tbest: 0.8491124 (284)\ttotal: 6.7s\tremaining: 1m\n",
      "350:\tlearn: 0.8505564\ttest: 0.8507795\tbest: 0.8507795 (350)\ttotal: 7.81s\tremaining: 59s\n",
      "400:\tlearn: 0.8551625\ttest: 0.8507795\tbest: 0.8514116 (380)\ttotal: 8.92s\tremaining: 57.8s\n",
      "450:\tlearn: 0.8574616\ttest: 0.8507795\tbest: 0.8516320 (430)\ttotal: 10s\tremaining: 56.6s\n",
      "500:\tlearn: 0.8601276\ttest: 0.8522643\tbest: 0.8537491 (473)\ttotal: 11.1s\tremaining: 55.4s\n",
      "550:\tlearn: 0.8621762\ttest: 0.8543689\tbest: 0.8562919 (523)\ttotal: 12.2s\tremaining: 54.3s\n",
      "600:\tlearn: 0.8646232\ttest: 0.8543689\tbest: 0.8562919 (523)\ttotal: 13.3s\tremaining: 53.2s\n",
      "bestTest = 0.8562918838\n",
      "bestIteration = 523\n",
      "Shrink model to first 524 iterations.\n",
      "MCC: 0.6872\n",
      "Train: 11826, Validation: 1314, Test: 1461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ledneva/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:47<00:00,  1.97it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:05<00:00,  2.04it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:05<00:00,  2.07it/s]\n",
      "Warning: less than 75% GPU memory available for training. Free: 10381.125 Total: 16269.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.7529952\ttest: 0.7460757\tbest: 0.7460757 (0)\ttotal: 25.3ms\tremaining: 1m 16s\n",
      "50:\tlearn: 0.8340607\ttest: 0.8251121\tbest: 0.8275246 (46)\ttotal: 1.19s\tremaining: 1m 8s\n",
      "100:\tlearn: 0.8444101\ttest: 0.8377897\tbest: 0.8377897 (98)\ttotal: 2.31s\tremaining: 1m 6s\n",
      "150:\tlearn: 0.8509370\ttest: 0.8413547\tbest: 0.8416370 (146)\ttotal: 3.44s\tremaining: 1m 4s\n",
      "200:\tlearn: 0.8565668\ttest: 0.8449956\tbest: 0.8464951 (189)\ttotal: 4.55s\tremaining: 1m 3s\n",
      "250:\tlearn: 0.8629218\ttest: 0.8447205\tbest: 0.8464951 (189)\ttotal: 5.66s\tremaining: 1m 2s\n",
      "300:\tlearn: 0.8666029\ttest: 0.8467671\tbest: 0.8467671 (275)\ttotal: 6.77s\tremaining: 1m\n",
      "350:\tlearn: 0.8702144\ttest: 0.8492908\tbest: 0.8492908 (326)\ttotal: 7.88s\tremaining: 59.5s\n",
      "400:\tlearn: 0.8720008\ttest: 0.8510638\tbest: 0.8523431 (376)\ttotal: 8.99s\tremaining: 58.3s\n",
      "450:\tlearn: 0.8742355\ttest: 0.8477876\tbest: 0.8523431 (376)\ttotal: 10.1s\tremaining: 57.1s\n",
      "bestTest = 0.8523430592\n",
      "bestIteration = 376\n",
      "Shrink model to first 377 iterations.\n",
      "MCC: 0.7287\n"
     ]
    }
   ],
   "source": [
    "# –°–ø–∏—Å–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –±—É–¥–µ—Ç –∏–∑–º–µ—Ä—è—Ç—å—Å—è –º–µ—Ç—Ä–∏–∫–∞ –≤–æ –≤—Ä–µ–º—è DNAChallenge\n",
    "# –í –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ 18 –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –æ–≥—Ä–∞–Ω–∏—á–∏–º—Å—è –ø—è—Ç—å—é\n",
    "\n",
    "# –ï—Å–ª–∏ –ø—Ä–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –æ—à–∏–±–∫–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å DNA-BERT-2, –≤–æ–∑–º–æ–∂–Ω–æ, –ø–æ–ª–µ–∑–Ω–æ–π –æ–∫–∞–∂–µ—Ç—Å—è —Å—Å—ã–ª–∫–∞\n",
    "# https://huggingface.co/mosaicml/mpt-7b-storywriter/discussions/10\n",
    "\n",
    "DATASETS = [\n",
    "    (\"promoter_all\", \"F1\"),\n",
    "    (\"enhancers\", \"MCC\"),\n",
    "    (\"splice_sites_all\", \"Accuracy\"),\n",
    "    (\"H3\", \"MCC\"),\n",
    "    (\"H4\", \"MCC\")\n",
    "]\n",
    "\n",
    "result_metrics = {}\n",
    "\n",
    "for dataset, metric in DATASETS:\n",
    "    result_metrics[dataset] = classify_with_dnabert(dataset, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4fd826c-1d43-495d-8e1d-3c0935027a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics per dataset:\n",
      "\n",
      "promoter_all     : 0.9249\n",
      "enhancers        : 0.4453\n",
      "splice_sites_all : 0.4607\n",
      "H3               : 0.6872\n",
      "H4               : 0.7287\n",
      "\n",
      "----------------------------\n",
      "Average          : 0.6493\n"
     ]
    }
   ],
   "source": [
    "# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —à–∏—Ä–∏–Ω–∞ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è\n",
    "max_name_len = max(len(name) for name in result_metrics.keys())\n",
    "line_format = f\"{{:<{max_name_len}}} : {{:.4f}}\"\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Ç–∞–±–ª–∏—Ü—É\n",
    "print(\"Metrics per dataset:\\n\")\n",
    "for dataset, score in result_metrics.items():\n",
    "    print(line_format.format(dataset, score))\n",
    "\n",
    "# –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "mean_score = sum(result_metrics.values()) / len(result_metrics)\n",
    "print(\"\\n\" + \"-\" * (max_name_len + 12))\n",
    "print(f\"{'Average'.ljust(max_name_len)} : {mean_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5e97ea-b5c4-4723-8e8a-4d39e8408c51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
